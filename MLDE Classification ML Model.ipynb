{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from urllib.request import urlopen \n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input uploading & Parameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_root = os.environ['WORKSPACE_BUCKET']\n",
    "print(bucket_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the path to file by running gsutil commandline tool.\n",
    "\n",
    "!gsutil ls gs://\n",
    "    \n",
    "!gsutil ls gs://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fileName = \"INPUT.csv\"\n",
    "\n",
    "## use the above file location information in the bucket\n",
    "bucket_loc = \"/DIRECTORY/INPUT.csv\"\n",
    "!gsutil cp gs://INPUT.csv INPUT.csv\n",
    "    \n",
    "input_df = pd.read_csv(fileName)\n",
    "print (\"(INFO) input uploaded\\n\")\n",
    "\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 'id_number' as our index\n",
    "input_df.set_index('id', inplace=True) \n",
    "# Converted to binary to help later on with models and plots\n",
    "input_df['Binding'] = input_df['Binding'].map({'B':1, 'N':0})\n",
    "\n",
    "print (\"(INFO) input\\n\")\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(\"Here's the dimensions of input:\\n\", \n",
    "     input_df.shape)\n",
    "print(\"Here's the data types:\\n\",\n",
    "     input_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_df.iloc[:, input_df.columns != 'Binding']\n",
    "y = input_df.iloc[:, input_df.columns == 'Binding']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (input_df['Binding'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.head(), y_train.head(), X_test.head(), y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random state for reproducibility\n",
    "fit_rf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "start = time.time()\n",
    "\n",
    "param_dist = {'max_depth': [2, 3, 4],\n",
    "              'bootstrap': [True, False],\n",
    "              'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "cv_rf = GridSearchCV(fit_rf, cv = 5,\n",
    "                     param_grid=param_dist, \n",
    "                     n_jobs = 3)\n",
    "\n",
    "cv_rf.fit(X_train, y_train.values.ravel())\n",
    "print('Best Parameters using grid search: \\n', \n",
    "      cv_rf.best_params_)\n",
    "end = time.time()\n",
    "print('Time taken in grid search: {0: .2f}'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set best parameters given by grid search \n",
    "fit_rf.set_params(criterion = 'entropy',\n",
    "                  max_features = 'auto', \n",
    "                  max_depth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rf.set_params(warm_start=True, \n",
    "                  oob_score=True)\n",
    "\n",
    "min_estimators = 15\n",
    "max_estimators = 1000\n",
    "\n",
    "error_rate = {}\n",
    "\n",
    "for i in range(min_estimators, max_estimators + 1):\n",
    "    fit_rf.set_params(n_estimators=i)\n",
    "    fit_rf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    oob_error = 1 - fit_rf.oob_score_\n",
    "    error_rate[i] = oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to a pandas series for easy plotting \n",
    "oob_series = pd.Series(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## test_size = 0.25, cv=5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.set_facecolor('#fafafa')\n",
    "\n",
    "oob_series.plot(kind='line',\n",
    "                color = 'red')\n",
    "plt.axhline(0.15, \n",
    "            color='#875FDB',\n",
    "           linestyle='--')\n",
    "plt.axhline(0.1, \n",
    "            color='#875FDB',\n",
    "           linestyle='--')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('OOB Error Rate')\n",
    "plt.title('OOB Error Rate Across various Forest sizes \\n(From 15 to 1000 trees)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test_size = 0.2, cv=10 results\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.set_facecolor('#fafafa')\n",
    "\n",
    "oob_series.plot(kind='line',\n",
    "                color = 'red')\n",
    "plt.axhline(0.15, \n",
    "            color='#875FDB',\n",
    "           linestyle='--')\n",
    "plt.axhline(0.1, \n",
    "            color='#875FDB',\n",
    "           linestyle='--')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('OOB Error Rate')\n",
    "plt.title('OOB Error Rate Across various Forest sizes \\n(From 15 to 1000 trees)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rf.set_params(n_estimators=400,\n",
    "                  bootstrap = True,\n",
    "                  warm_start=False, \n",
    "                  oob_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rf.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_importance(fit):\n",
    "    \"\"\"\n",
    "    Purpose\n",
    "    ----------\n",
    "    Checks if model is fitted CART model then produces variable importance\n",
    "    and respective indices in dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    * fit:  Fitted model containing the attribute feature_importances_\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    Dictionary containing arrays with importance score and index of columns\n",
    "    ordered in descending order of importance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not hasattr(fit, 'fit'):\n",
    "            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n",
    "\n",
    "        # Captures whether the model has been trained\n",
    "        if not vars(fit)[\"estimators_\"]:\n",
    "            return print(\"Model does not appear to be trained.\")\n",
    "    except KeyError:\n",
    "        print(\"Model entered does not contain 'estimators_' attribute.\")\n",
    "\n",
    "    importances = fit.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    return {'importance': importances,\n",
    "            'index': indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp_rf = variable_importance(fit_rf)\n",
    "\n",
    "importances_rf = var_imp_rf['importance']\n",
    "\n",
    "indices_rf = var_imp_rf['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (var_imp_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_var_importance(importance, indices, name_index):\n",
    "    \"\"\"\n",
    "    Purpose\n",
    "    ----------\n",
    "    Prints dependent variable names ordered from largest to smallest\n",
    "    based on information gain for CART model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    * importance: Array returned from feature_importances_ for CART\n",
    "                models organized by dataframe index\n",
    "    * indices: Organized index of dataframe from largest to smallest\n",
    "                based on feature_importances_\n",
    "    * name_index: Name of columns included in model\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    Prints feature importance in descending order\n",
    "    \"\"\"\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(0, indices.shape[0]):\n",
    "        i = f\n",
    "        print(\"{0}. The feature '{1}' has a Mean Decrease in Impurity of {2:.5f}\"\n",
    "              .format(f + 1,\n",
    "                      name_index[indices[i]],\n",
    "                      importance[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (input_df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later use in CART models\n",
    "names_index = input_df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_var_importance(importances_rf, indices_rf, names_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(fit, training_set, class_set, estimator, print_results = True, n_splits=10):\n",
    "    \"\"\"\n",
    "    Purpose\n",
    "    ----------\n",
    "    Function helps automate cross validation processes while including \n",
    "    option to print metrics or store in variable\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit: Fitted model \n",
    "    training_set:  Data_frame containing 80% of original dataframe\n",
    "    class_set:     data_frame containing the respective target vaues \n",
    "                      for the training_set\n",
    "    print_results: Boolean, if true prints the metrics, else saves metrics as \n",
    "                      variables\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    scores.mean(): Float representing cross validation score\n",
    "    scores.std() / 2: Float representing the standard error (derived\n",
    "                from cross validation score's standard deviation)\n",
    "    \"\"\"\n",
    "    my_estimators = {\n",
    "    'rf': 'estimators_',\n",
    "    'nn': 'out_activation_',\n",
    "    'knn': '_fit_method'\n",
    "    }\n",
    "    try:\n",
    "        # Captures whether first parameter is a model\n",
    "        if not hasattr(fit, 'fit'):\n",
    "            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n",
    "\n",
    "        # Captures whether the model has been trained\n",
    "        if not vars(fit)[my_estimators[estimator]]:\n",
    "            return print(\"Model does not appear to be trained.\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n",
    "\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n",
    "        raise\n",
    "\n",
    "    n = KFold(n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(fit, \n",
    "                         training_set, \n",
    "                         class_set, \n",
    "                         cv = n)\n",
    "    if print_results:\n",
    "        for i in range(0, len(scores)):\n",
    "            print(\"Cross validation run {0}: {1: 0.3f}\".format(i, scores[i]))\n",
    "        print(\"Accuracy: {0: 0.3f} (+/- {1: 0.3f})\"\\\n",
    "              .format(scores.mean(), scores.std() / 2))\n",
    "       \n",
    "        \n",
    "    else:\n",
    "        return scores.mean(), scores.std() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_val_metrics(fit_rf, \n",
    "                  X_train, \n",
    "                  y_train.values.ravel(), \n",
    "                  'rf',\n",
    "                  print_results = True, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conf_mat(test_class_set, predictions):\n",
    "    \"\"\"Function returns confusion matrix comparing two arrays\"\"\"\n",
    "    if (len(test_class_set.shape) != len(predictions.shape) == 1):\n",
    "        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n",
    "    elif (test_class_set.shape != predictions.shape):\n",
    "        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n",
    "    else:\n",
    "        # Set Metrics\n",
    "        test_crosstb_comp = pd.crosstab(index = test_class_set,\n",
    "                                        columns = predictions)\n",
    "        # Changed for Future deprecation of as_matrix\n",
    "        test_crosstb = test_crosstb_comp.values\n",
    "        return test_crosstb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fit_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = create_conf_mat(y_test.values.ravel(), y_pred.ravel())\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.title('Actual vs. Predicted Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rf = fit_rf.score(X_test, y_test)\n",
    "\n",
    "print(\"Here is our mean accuracy on the test set:\\n {0:.3f}\"\\\n",
    "      .format(accuracy_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we calculate the test error rate!\n",
    "test_error_rate_rf = 1 - accuracy_rf\n",
    "print(\"The test error rate for our model is:\\n {0: .4f}\"\\\n",
    "      .format(test_error_rate_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_report(y_pred, y_test, alg_name):\n",
    "    \"\"\"\n",
    "    Purpose\n",
    "    ----------\n",
    "    Function helps automate the report generated by the\n",
    "    sklearn package. Useful for multiple model comparison\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    predictions: The predictions made by the algorithm used\n",
    "    alg_name: String containing the name of the algorithm used\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    Returns classification report generated from sklearn. \n",
    "    \"\"\"\n",
    "    print('Classification Report for {0}:'.format(alg_name))\n",
    "    print(classification_report(y_pred, \n",
    "            y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_report = print_class_report( y_test, y_pred, 'Random Forest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We grab the second array from the output which corresponds to\n",
    "# to the predicted probabilites of positive classes \n",
    "# Ordered wrt fit.classes_ in our case [0, 1] where 1 is our positive class\n",
    "predictions_prob = fit_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(y_test, predictions_prob, pos_label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_rf = auc(fpr2, tpr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, auc, estimator, xlim=None, ylim=None):\n",
    "    \"\"\"\n",
    "    Purpose\n",
    "    ----------\n",
    "    Function creates ROC Curve for respective model given selected parameters.\n",
    "    Optional x and y limits to zoom into graph\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    * fpr: Array returned from sklearn.metrics.roc_curve for increasing\n",
    "            false positive rates\n",
    "    * tpr: Array returned from sklearn.metrics.roc_curve for increasing\n",
    "            true positive rates\n",
    "    * auc: Float returned from sklearn.metrics.auc (Area under Curve)\n",
    "    * estimator: String represenation of appropriate model, can only contain the\n",
    "    following: ['knn', 'rf', 'nn']\n",
    "    * xlim: Set upper and lower x-limits\n",
    "    * ylim: Set upper and lower y-limits\n",
    "    \"\"\"\n",
    "    my_estimators = {'knn': ['Kth Nearest Neighbor', 'deeppink'],\n",
    "              'rf': ['Random Forest', 'red'],\n",
    "              'nn': ['Neural Network', 'purple']}\n",
    "\n",
    "    try:\n",
    "        plot_title = my_estimators[estimator][0]\n",
    "        color_value = my_estimators[estimator][1]\n",
    "    except KeyError as e:\n",
    "        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n",
    "\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n",
    "        raise\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.set_facecolor('#fafafa')\n",
    "\n",
    "    plt.plot(fpr, tpr,\n",
    "             color=color_value,\n",
    "             linewidth=1)\n",
    "    plt.title('ROC Curve For {0} (AUC = {1: 0.3f})'\\\n",
    "              .format(plot_title, auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], lw=2) # Add Diagonal line\n",
    "    plt.plot([0, 0], [1, 0], lw=2, color = 'black')\n",
    "    plt.plot([1, 0], [1, 1], lw=2, color = 'black')\n",
    "    if xlim is not None:\n",
    "        plt.xlim(*xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr2, tpr2, auc_rf, 'rf',\n",
    "               xlim=(-0.01, 1.05), \n",
    "               ylim=(0.001, 1.05))\n",
    "\n",
    "plot_roc_curve(fpr2, tpr2, auc_rf, 'rf', \n",
    "               xlim=(-0.01, 0.2), \n",
    "               ylim=(0.85, 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2 = \"PREDICTION_LIBRARY.csv\"\n",
    "\n",
    "## use the above file location information in the bucket\n",
    "bucket_loc = \"DIRECTORY/PREDICTION_LIBRARY.csv\"\n",
    "!gsutil cp gs://PREDICTION_LIBRARY.csv PREDICTION_LIBRARY.csv\n",
    "predict_df = pd.read_csv(input_2)\n",
    "# Setting 'id_number' as our index\n",
    "predict_df.set_index('id', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Used trained-ML model to predict classification on PREDICTION_LIBRARY.csv\n",
    "predict_out = fit_rf.predict(predict_df)\n",
    "print(predict_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "72px",
    "width": "292px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
